{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('ml': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "e58d3b4a238b69da25db37debc8e7c035df90db97ecce5d5d0275e11224d75ee"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics as me\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dir = \"filter\"\n",
    "\n",
    "def dataset_split(df):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.5, random_state=0)\n",
    "    x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=0)\n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "\n",
    "datasets = ['ExoVarFiltered', 'HumVarFiltered', 'predictSNPSelected', 'SwissVarSelected', 'VariBenchSelected']\n",
    "dfs = {}\n",
    "\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for file_ in files:\n",
    "        dataset = file_.split(\".\")[0]\n",
    "        df = pd.read_csv(os.path.join(root, file_))\n",
    "        dfs[dataset] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]], index=[\"Dataset\", \"Dataset-train\", \"Dataset-vaild\" ,\"Dataset-test\",\"ExtraDataset\"], columns=[\"Total\", \"Deleterious\", \"Benign\"])\n",
    "count_df.iat[0, 0] = len(df[\"True Label\"])\n",
    "count_df.iat[4, 0] = len(extra_df[\"True Label\"])\n",
    "df_count = df[\"True Label\"].value_counts().to_dict()\n",
    "extra_df_count = extra_df[\"True Label\"].value_counts().to_dict()\n",
    "count_df.iat[0, 1] = df_count[0]\n",
    "count_df.iat[0, 2] = df_count[1]\n",
    "count_df.iat[4, 1] = extra_df_count[0]\n",
    "count_df.iat[4, 2] = extra_df_count[1]\n",
    "count_df.iat[1, 0] = len(y_train)\n",
    "count_df.iat[1, 1] = y_train.value_counts().to_dict()[0]\n",
    "count_df.iat[1, 2] = y_train.value_counts().to_dict()[1]\n",
    "count_df.iat[2, 0] = len(y_valid)\n",
    "count_df.iat[2, 1] = y_valid.value_counts().to_dict()[0]\n",
    "count_df.iat[2, 2] = y_valid.value_counts().to_dict()[1]\n",
    "count_df.iat[3, 0] = len(y_test)\n",
    "count_df.iat[3, 1] = y_test.value_counts().to_dict()[0]\n",
    "count_df.iat[3, 2] = y_test.value_counts().to_dict()[1]\n",
    "print(count_df)\n",
    "import dataframe_image as dfi\n",
    "\n",
    "df_styled = count_df.style.background_gradient()\n",
    "dfi.export(df_styled, \"count_df.png\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从5个数据集中抽出相同比例的训练、验证和测试集\n",
    "x_train_merged = pd.DataFrame()\n",
    "x_valid_merged = pd.DataFrame()\n",
    "x_test_merged = pd.DataFrame()\n",
    "y_train_merged = pd.Series()\n",
    "y_valid_merged = pd.Series()\n",
    "y_test_merged = pd.Series()\n",
    "for dataset in ['ExoVarFiltered', 'HumVarFiltered', 'predictSNPSelected', 'VariBenchSelected']:\n",
    "    df = dfs[dataset]\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.4, random_state=0)\n",
    "    x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=0)\n",
    "    x_train_merged = pd.concat([x_train_merged, x_train])\n",
    "    x_valid_merged = pd.concat([x_valid_merged, x_valid])\n",
    "    x_test_merged = pd.concat([x_test_merged, x_test])\n",
    "    y_train_merged = pd.concat([y_train_merged, y_train])\n",
    "    y_valid_merged = pd.concat([y_valid_merged, y_valid])\n",
    "    y_test_merged = pd.concat([y_test_merged, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gridSearchCV选择最优参数\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "num_leaves = [ _ for _ in range(11, 60, 2)]\n",
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "n_estimators = range(10, 150, 10)\n",
    "max_depths = range(-1, 10, 1)\n",
    "params_test = {\n",
    "    \"num_leaves\" : num_leaves,\n",
    "    \"learning_rate\": learning_rates,\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_depth\": max_depths\n",
    "}\n",
    "\n",
    "classifier = LGBMClassifier(objective=\"binary\", metrics='auc', verbose=-1)\n",
    "gsCV = GridSearchCV(classifier, params_test, verbose=4)\n",
    "gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_leaves\" : 31,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": 'multi_logloss',\n",
    "    \"max_bin\": 50,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"is_unbalance\": True,\n",
    "    \"num_class\": 2\n",
    "    # \"silent\": 1,  # 信息输出设置成1则没有信息输出\n",
    "}\n",
    "\n",
    "print(\"training begins\")\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[valid_data, valid_data_2, valid_data_3], early_stopping_rounds=10) \n",
    "bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "bst.save_model('mutationscore.model')\n",
    "print(\"training finishes\")\n"
   ]
  },
  {
   "source": [
    "# 调参"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 准备数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并后的数据集\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"filter/merged_with_labels.csv\")\n",
    "features = ['Chr', 'Start', 'End', 'Ref', 'Alt', 'SIFT_score', 'Polyphen2_HDIV_score', 'Polyphen2_HVAR_score','LRT_score', 'MutationTaster_score', 'MutationAssessor_score',  'FATHMM_score', 'RadialSVM_score', 'VEST3_score', 'CADD_phred', 'A_Ref', 'A_Alt']\n",
    "\n",
    "labels = [\"SIFT_Label\", \"Polyphen2_HDIV_Label\", \"Polyphen2_HVAR_Label\", \"LRT_Label\", \"MutationTaster_Label\", \"MutationAssessor_Label\", \"FATHMM_Label\", \"RadialSVM_Label\", \"VEST3_score\", \"CADD_phred\"]\n",
    "# df = df[features]\n",
    " \n",
    "\n",
    "x_train_all, x_valid, y_train, y_valid = train_test_split(df, df.iloc[:, 0], test_size=0.4, random_state=0)\n",
    "x_valid_all, x_test_all, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=0)\n",
    "\n",
    "# only need some features as input of classifier\n",
    "x_train = x_train_all[features]\n",
    "x_valid = x_valid_all[features]\n",
    "x_test = x_valid_all[features]\n",
    "\n",
    "# remove those Nan from others classifier's results\n",
    "x_test_nan_removed = x_test_all.dropna(subset=labels)\n",
    "\n",
    "swissvar_test_all = pd.read_csv(\"filter/swissvar_with_labels.csv\")\n",
    "# swissvar_test_all = swissvar_test_all.dropna(subset=labels)\n",
    "# swissvar_test_input = swissvar_test_all[features]\n",
    "\n",
    "comparators = ['MCAP', 'REVEL', 'MVP_score','DANN_score', 'GERP++_RS']\n",
    "swissvar_compare = swissvar_test_all.dropna(subset=comparators)\n",
    "swissvar_input = swissvar_compare[features]\n",
    "# swissvar_output = optimized_classifier.predict_proba(swissvar_input)\n",
    "# swissvar_output = swissvar_output[:, 1]\n",
    "swissvar_label = swissvar_compare[\"True Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(x_train, y_train)\n",
    "train_data.create_valid(swissvar_input, swissvar_label)"
   ]
  },
  {
   "source": [
    "## n_estimators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# 使用gridSearchCV选择最优参数\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# num_leaves = [ _ for _ in range(11, 60, 2)]\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "# n_estimators = range(10, 150, 10)\n",
    "# max_depths = range(-1, 10, 1)\n",
    "params = {\n",
    "    # \"num_leaves\" : num_leaves,\n",
    "    \"learning_rate\": learning_rates,\n",
    "    # \"n_estimators\": n_estimators,\n",
    "    # \"max_depth\": max_depths\n",
    "    # \"boosting\" : [\"gbdt\", \"rf\", \"dart\", \"goss\"]\n",
    "}\n",
    "\n",
    "params = {    \n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'learning_rate':0.1,\n",
    "          'num_leaves':31, \n",
    "          'max_depth': 5,   \n",
    "    }\n",
    "\n",
    "cv_results = lgb.cv(params, train_data)\n",
    "print(len(cv_results['auc-mean']))\n",
    "\n",
    "# n_estimators= 100\n",
    "# classifier = LGBMClassifier(objective=\"binary\", metrics='auc', boosting=\"gbdt\")\n",
    "# gsCV = GridSearchCV(classifier, params, verbose=4)\n",
    "# gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])"
   ]
  },
  {
   "source": [
    "## max_depth 和num_leaves"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\" : range(5, 10, 1),\n",
    "    'num_leaves': range(110, 170, 10)\n",
    "}\n",
    "\n",
    "classifier = LGBMClassifier(objective=\"binary\", metrics='auc', n_estimators=100, boosting=\"gbdt\", learning_rate=0.1)\n",
    "gsCV = GridSearchCV(classifier, params, verbose=4)\n",
    "gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])\n",
    "\n",
    "# max_depth=8 num_leaves=150\n"
   ]
  },
  {
   "source": [
    "min_data_in_leaf 和 min_sum_hessian_in_leaf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'min_child_samples': [18, 19, 20, 21, 22],\n",
    "    'min_child_weight':[0.001, 0.002]\n",
    "}\n",
    "\n",
    "classifier = LGBMClassifier(boosting_type=\"gbdt\", num_leaves=150, max_depth=8, learning_rate=0.1, n_estimators=100, metrics='auc', bagging_fraction = 0.8, feature_fraction = 0.8)\n",
    "gsCV = GridSearchCV(classifier, params, verbose=4)\n",
    "gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])\n",
    "\n",
    "# min_child_samples=20 min_child_weight=0.001"
   ]
  },
  {
   "source": [
    "## bagging_fraction和feature_fraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "params ={\n",
    "    'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "classifier = LGBMClassifier(boosting_type=\"gbdt\", num_leaves=150, max_depth=8, learning_rate=0.1, n_estimators=100, metrics='auc', bagging_fraction = 0.8, feature_fraction = 0.8, min_child_samples=20, min_child_weight=0.001)\n",
    "gsCV = GridSearchCV(classifier, params, verbose=4)\n",
    "gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])\n",
    "\n",
    "# bagging_fraction': 0.6, 'feature_fraction': 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "params ={\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "classifier = LGBMClassifier(boosting_type=\"gbdt\", num_leaves=150, max_depth=8, learning_rate=0.1, n_estimators=100, metrics='auc', min_child_samples=20, min_child_weight=0.001, bagging_fraction=0.6, feature_fraction=0.5)\n",
    "gsCV = GridSearchCV(classifier, params, verbose=4)\n",
    "gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])\n",
    "\n",
    "# 'reg_alpha': 0.08, 'reg_lambda': 0.3"
   ]
  },
  {
   "source": [
    "print(gsCV.best_params_)\n",
    "print(gsCV.best_score_)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}