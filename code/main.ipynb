{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('ml': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "e58d3b4a238b69da25db37debc8e7c035df90db97ecce5d5d0275e11224d75ee"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "E:\\Envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (17,69) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "E:\\Envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (46,59) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "E:\\Envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (8,46,59) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "E:\\Envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics as me\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dir = \"filter\"\n",
    "\n",
    "def dataset_split(df):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.5, random_state=0)\n",
    "    x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=0)\n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "\n",
    "datasets = ['ExoVarFiltered', 'HumVarFiltered', 'predictSNPSelected', 'SwissVarSelected', 'VariBenchSelected']\n",
    "dfs = {}\n",
    "\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for file_ in files:\n",
    "        dataset = file_.split(\".\")[0]\n",
    "        df = pd.read_csv(os.path.join(root, file_))\n",
    "        dfs[dataset] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a7ce4cb9ed9e>, line 40)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-a7ce4cb9ed9e>\"\u001b[1;36m, line \u001b[1;32m40\u001b[0m\n\u001b[1;33m    elif mt == \"N\" of mt==\"P\":\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 把annovar的结果转为0 1，其中0为deleterious，1为benign\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SIFT\n",
    "def trans_SIFT(row):\n",
    "    sift = row[\"SIFT_pred\"]\n",
    "    if sift == \"D\":\n",
    "        return 0\n",
    "    elif sift == \"T\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# Polyphen2_HDIV\n",
    "def trans_Polyphen2_HDIV(row):\n",
    "    pass\n",
    "\n",
    "# Polyphen2_HVAR\n",
    "def trans_Polyphen2_HVAR(row):\n",
    "    pass\n",
    "\n",
    "# LRT\n",
    "def trans_LRT(row):\n",
    "    lrt = row[\"LRT_pred\"]\n",
    "    if lrt == \"D\":\n",
    "        return 0\n",
    "    elif lrt == \"N\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# MutationTaster\n",
    "def trans_MutationTaster(row):\n",
    "    mt = row[\"MutationTaster_pred\"]\n",
    "    if mt == \"A\" or mt == \"D\":\n",
    "        return 0\n",
    "    elif mt == \"N\" or mt==\"P\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# MutationAssessor\n",
    "def trans_MutationAssessor(row):\n",
    "    ma = row[\"MutationAssessor_pred\"]\n",
    "    if ma == \"H\" or ma == \"M\":\n",
    "        return 0\n",
    "    elif ma == \"L\" or ma == \"N\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# FATHMM\n",
    "def trans_FATHMM(row):\n",
    "    mm = row[\"FATHMM_pred\"]\n",
    "    if mm == \"D\":\n",
    "        return 0\n",
    "    elif mm == \"T\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# Radial\n",
    "def trans_Radial(row):\n",
    "    ra = row[\"RadialSVM_pred\"]\n",
    "    if ra == \"D\":\n",
    "        return 0\n",
    "    elif ra == \"T\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# LR\n",
    "def trans_LR(row):\n",
    "    lrt = row[\"LR_pred\"]\n",
    "    if lrt == \"D\":\n",
    "        return 0\n",
    "    elif lrt == \"N\":\n",
    "        return 1\n",
    "    else:\n",
    "        return np.NaN\n",
    "\n",
    "# VEST3\n",
    "def trans_VEST3(row):\n",
    "    pass\n",
    "\n",
    "# CADD\n",
    "def trans_CADD(row):\n",
    "    cadd = row[\"CADD_phred\"]\n",
    "    if np.isnan():\n",
    "        return np.NaN\n",
    "    else:\n",
    "        if cadd<=15:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "df = pd.read_csv(\"filter/merged.csv\")\n",
    "df.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "E:\\Envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (8,46,59) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# 合并后的数据集\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"filter/merged.csv\")\n",
    "features = ['True Label', 'Chr', 'Start', 'End', 'Ref', 'Alt', 'SIFT_score', 'SIFT_pred', 'Polyphen2_HDIV_score', \"Polyphen2_HDIV_pred\", 'Polyphen2_HVAR_score', \"Polyphen2_HVAR_pred\" ,'LRT_score', 'LRT_pred', 'MutationTaster_score', 'MutationTaster_pred', 'MutationAssessor_score', 'MutationAssessor_pred', 'FATHMM_score', 'FATHMM_pred', 'RadialSVM_score', 'LR_score', 'VEST3_score', 'CADD_phred', 'A_Ref', 'A_Alt']\n",
    "df = df[features]\n",
    " \n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.4, random_state=0)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=0)\n",
    "\n",
    "extra_testset = pd.read_csv(\"filter/SwissVarSelected.hg19_multianno.csv\")\n",
    "extra_df = extra_testset[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               Total  Deleterious  Benign\nDataset        70268        34580   35688\nDataset-train  42160        20659   21501\nDataset-vaild  14054         6919    7135\nDataset-test   14054         7002    7052\nExtraDataset   12144         7688    4456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_df = pd.DataFrame([[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]], index=[\"Dataset\", \"Dataset-train\", \"Dataset-vaild\" ,\"Dataset-test\",\"ExtraDataset\"], columns=[\"Total\", \"Deleterious\", \"Benign\"])\n",
    "count_df.iat[0, 0] = len(df[\"True Label\"])\n",
    "count_df.iat[4, 0] = len(extra_df[\"True Label\"])\n",
    "df_count = df[\"True Label\"].value_counts().to_dict()\n",
    "extra_df_count = extra_df[\"True Label\"].value_counts().to_dict()\n",
    "count_df.iat[0, 1] = df_count[0]\n",
    "count_df.iat[0, 2] = df_count[1]\n",
    "count_df.iat[4, 1] = extra_df_count[0]\n",
    "count_df.iat[4, 2] = extra_df_count[1]\n",
    "count_df.iat[1, 0] = len(y_train)\n",
    "count_df.iat[1, 1] = y_train.value_counts().to_dict()[0]\n",
    "count_df.iat[1, 2] = y_train.value_counts().to_dict()[1]\n",
    "count_df.iat[2, 0] = len(y_valid)\n",
    "count_df.iat[2, 1] = y_valid.value_counts().to_dict()[0]\n",
    "count_df.iat[2, 2] = y_valid.value_counts().to_dict()[1]\n",
    "count_df.iat[3, 0] = len(y_test)\n",
    "count_df.iat[3, 1] = y_test.value_counts().to_dict()[0]\n",
    "count_df.iat[3, 2] = y_test.value_counts().to_dict()[1]\n",
    "print(count_df)\n",
    "import dataframe_image as dfi\n",
    "\n",
    "df_styled = count_df.style.background_gradient()\n",
    "dfi.export(df_styled, \"count_df.png\", fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从5个数据集中抽出相同比例的训练、验证和测试集\n",
    "x_train_merged = pd.DataFrame()\n",
    "x_valid_merged = pd.DataFrame()\n",
    "x_test_merged = pd.DataFrame()\n",
    "y_train_merged = pd.Series()\n",
    "y_valid_merged = pd.Series()\n",
    "y_test_merged = pd.Series()\n",
    "for dataset in ['ExoVarFiltered', 'HumVarFiltered', 'predictSNPSelected', 'VariBenchSelected']:\n",
    "    df = dfs[dataset]\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.4, random_state=0)\n",
    "    x_valid, x_test, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, random_state=0)\n",
    "    x_train_merged = pd.concat([x_train_merged, x_train])\n",
    "    x_valid_merged = pd.concat([x_valid_merged, x_valid])\n",
    "    x_test_merged = pd.concat([x_test_merged, x_test])\n",
    "    y_train_merged = pd.concat([y_train_merged, y_train])\n",
    "    y_valid_merged = pd.concat([y_valid_merged, y_valid])\n",
    "    y_test_merged = pd.concat([y_test_merged, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用gridSearchCV选择最优参数\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "num_leaves = [ _ for _ in range(11, 60, 2)]\n",
    "learning_rates = [0.1, 0.05, 0.01]\n",
    "n_estimators = range(10, 150, 10)\n",
    "max_depths = range(-1, 10, 1)\n",
    "params_test = {\n",
    "    \"num_leaves\" : num_leaves,\n",
    "    \"learning_rate\": learning_rates,\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_depth\": max_depths\n",
    "}\n",
    "\n",
    "classifier = LGBMClassifier(objective=\"binary\", metrics='auc', verbose=-1)\n",
    "gsCV = GridSearchCV(classifier, params_test, verbose=4)\n",
    "gsCV.fit(x_train, y_train, categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "E:\\Envs\\ml\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "E:\\Envs\\ml\\lib\\site-packages\\lightgbm\\basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "E:\\Envs\\ml\\lib\\site-packages\\lightgbm\\basic.py:1706: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['A_Alt', 'A_Ref', 'Alt', 'Chr', 'Ref']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "E:\\Envs\\ml\\lib\\site-packages\\lightgbm\\basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "E:\\Envs\\ml\\lib\\site-packages\\lightgbm\\basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n",
      "[LightGBM] [Warning] num_iterations is set=200, num_round=200 will be ignored. Current value: num_iterations=200\n",
      "[1]\tvalid_0's auc: 0.974165\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's auc: 0.977472\n",
      "[3]\tvalid_0's auc: 0.97764\n",
      "[4]\tvalid_0's auc: 0.97829\n",
      "[5]\tvalid_0's auc: 0.978517\n",
      "[6]\tvalid_0's auc: 0.978707\n",
      "[7]\tvalid_0's auc: 0.978875\n",
      "[8]\tvalid_0's auc: 0.979475\n",
      "[9]\tvalid_0's auc: 0.98\n",
      "[10]\tvalid_0's auc: 0.98013\n",
      "[11]\tvalid_0's auc: 0.980316\n",
      "[12]\tvalid_0's auc: 0.980483\n",
      "[13]\tvalid_0's auc: 0.980656\n",
      "[14]\tvalid_0's auc: 0.980823\n",
      "[15]\tvalid_0's auc: 0.980967\n",
      "[16]\tvalid_0's auc: 0.981143\n",
      "[17]\tvalid_0's auc: 0.981331\n",
      "[18]\tvalid_0's auc: 0.981435\n",
      "[19]\tvalid_0's auc: 0.981515\n",
      "[20]\tvalid_0's auc: 0.981671\n",
      "[21]\tvalid_0's auc: 0.98176\n",
      "[22]\tvalid_0's auc: 0.981831\n",
      "[23]\tvalid_0's auc: 0.981919\n",
      "[24]\tvalid_0's auc: 0.982065\n",
      "[25]\tvalid_0's auc: 0.982121\n",
      "[26]\tvalid_0's auc: 0.982242\n",
      "[27]\tvalid_0's auc: 0.982281\n",
      "[28]\tvalid_0's auc: 0.982366\n",
      "[29]\tvalid_0's auc: 0.982539\n",
      "[30]\tvalid_0's auc: 0.982699\n",
      "[31]\tvalid_0's auc: 0.982824\n",
      "[32]\tvalid_0's auc: 0.982936\n",
      "[33]\tvalid_0's auc: 0.983056\n",
      "[34]\tvalid_0's auc: 0.983319\n",
      "[35]\tvalid_0's auc: 0.983457\n",
      "[36]\tvalid_0's auc: 0.983515\n",
      "[37]\tvalid_0's auc: 0.98364\n",
      "[38]\tvalid_0's auc: 0.983747\n",
      "[39]\tvalid_0's auc: 0.98374\n",
      "[40]\tvalid_0's auc: 0.983868\n",
      "[41]\tvalid_0's auc: 0.983959\n",
      "[42]\tvalid_0's auc: 0.984065\n",
      "[43]\tvalid_0's auc: 0.98423\n",
      "[44]\tvalid_0's auc: 0.984351\n",
      "[45]\tvalid_0's auc: 0.984468\n",
      "[46]\tvalid_0's auc: 0.984539\n",
      "[47]\tvalid_0's auc: 0.98466\n",
      "[48]\tvalid_0's auc: 0.984759\n",
      "[49]\tvalid_0's auc: 0.984846\n",
      "[50]\tvalid_0's auc: 0.984907\n",
      "[51]\tvalid_0's auc: 0.984976\n",
      "[52]\tvalid_0's auc: 0.985005\n",
      "[53]\tvalid_0's auc: 0.985076\n",
      "[54]\tvalid_0's auc: 0.985105\n",
      "[55]\tvalid_0's auc: 0.985125\n",
      "[56]\tvalid_0's auc: 0.98519\n",
      "[57]\tvalid_0's auc: 0.985209\n",
      "[58]\tvalid_0's auc: 0.985285\n",
      "[59]\tvalid_0's auc: 0.985326\n",
      "[60]\tvalid_0's auc: 0.985363\n",
      "[61]\tvalid_0's auc: 0.985386\n",
      "[62]\tvalid_0's auc: 0.985359\n",
      "[63]\tvalid_0's auc: 0.985407\n",
      "[64]\tvalid_0's auc: 0.985409\n",
      "[65]\tvalid_0's auc: 0.985429\n",
      "[66]\tvalid_0's auc: 0.98551\n",
      "[67]\tvalid_0's auc: 0.985541\n",
      "[68]\tvalid_0's auc: 0.985556\n",
      "[69]\tvalid_0's auc: 0.985572\n",
      "[70]\tvalid_0's auc: 0.985577\n",
      "[71]\tvalid_0's auc: 0.985636\n",
      "[72]\tvalid_0's auc: 0.985688\n",
      "[73]\tvalid_0's auc: 0.985701\n",
      "[74]\tvalid_0's auc: 0.985754\n",
      "[75]\tvalid_0's auc: 0.985737\n",
      "[76]\tvalid_0's auc: 0.985764\n",
      "[77]\tvalid_0's auc: 0.985795\n",
      "[78]\tvalid_0's auc: 0.985818\n",
      "[79]\tvalid_0's auc: 0.985847\n",
      "[80]\tvalid_0's auc: 0.985866\n",
      "[81]\tvalid_0's auc: 0.985868\n",
      "[82]\tvalid_0's auc: 0.985908\n",
      "[83]\tvalid_0's auc: 0.985921\n",
      "[84]\tvalid_0's auc: 0.98594\n",
      "[85]\tvalid_0's auc: 0.985948\n",
      "[86]\tvalid_0's auc: 0.985952\n",
      "[87]\tvalid_0's auc: 0.985973\n",
      "[88]\tvalid_0's auc: 0.986011\n",
      "[89]\tvalid_0's auc: 0.986046\n",
      "[90]\tvalid_0's auc: 0.986101\n",
      "[91]\tvalid_0's auc: 0.986123\n",
      "[92]\tvalid_0's auc: 0.986134\n",
      "[93]\tvalid_0's auc: 0.986143\n",
      "[94]\tvalid_0's auc: 0.986193\n",
      "[95]\tvalid_0's auc: 0.986227\n",
      "[96]\tvalid_0's auc: 0.986246\n",
      "[97]\tvalid_0's auc: 0.986294\n",
      "[98]\tvalid_0's auc: 0.986293\n",
      "[99]\tvalid_0's auc: 0.986318\n",
      "[100]\tvalid_0's auc: 0.986326\n",
      "[101]\tvalid_0's auc: 0.986316\n",
      "[102]\tvalid_0's auc: 0.98634\n",
      "[103]\tvalid_0's auc: 0.986342\n",
      "[104]\tvalid_0's auc: 0.986306\n",
      "[105]\tvalid_0's auc: 0.986324\n",
      "[106]\tvalid_0's auc: 0.986333\n",
      "[107]\tvalid_0's auc: 0.986348\n",
      "[108]\tvalid_0's auc: 0.986343\n",
      "[109]\tvalid_0's auc: 0.986323\n",
      "[110]\tvalid_0's auc: 0.986313\n",
      "[111]\tvalid_0's auc: 0.986295\n",
      "[112]\tvalid_0's auc: 0.986301\n",
      "[113]\tvalid_0's auc: 0.98631\n",
      "[114]\tvalid_0's auc: 0.98631\n",
      "[115]\tvalid_0's auc: 0.986339\n",
      "[116]\tvalid_0's auc: 0.986307\n",
      "[117]\tvalid_0's auc: 0.986303\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's auc: 0.986348\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 7,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 140,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': 'binary',\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': True,\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0,\n",
       " 'is_unbalance': True,\n",
       " 'num_round': 200,\n",
       " 'metrics': 'auc'}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# 调参后，分类器使用最优参数\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "num_leaves = 31\n",
    "n_estimators = 140\n",
    "max_depth = 7\n",
    "learning_rate = 0.1 \n",
    "\n",
    "optimized_classifier = LGBMClassifier(objective='binary', num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, is_unbalance=True, num_round=200, metrics='auc')\n",
    "optimized_classifier.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], categorical_feature=[\"Chr\", \"Ref\", \"Alt\", \"A_Ref\", \"A_Alt\"], early_stopping_rounds=10)\n",
    "optimized_classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------report-----\n              precision    recall  f1-score   support\n\n           0       0.95      0.94      0.94      7002\n           1       0.94      0.95      0.95      7052\n\n    accuracy                           0.94     14054\n   macro avg       0.94      0.94      0.94     14054\nweighted avg       0.94      0.94      0.94     14054\n\n------roc_auc_score------\n0.9847025313082091\n-----accuracy-----\n0.9447844030169347\n------acccuracy------\n0.7467885375494071\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的分类器对测试数据进行分类\n",
    "\n",
    "import sklearn.metrics as me\n",
    "\n",
    "\n",
    "# 对测试集进行测试\n",
    "y_pred = optimized_classifier.predict(x_test)\n",
    "y_pred_prob = optimized_classifier.predict_proba(x_test)\n",
    "y_pred_prob_0 = y_pred_prob[:,1]\n",
    "print(\"------report-----\")\n",
    "print(me.classification_report(y_test, y_pred))\n",
    "print(\"------roc_auc_score------\")\n",
    "print(me.roc_auc_score(y_test, y_pred_prob_0))\n",
    "# thresholds = []\n",
    "# start = 0\n",
    "# for i in range(10):\n",
    "#         thresholds.append(round(start + i * 0.1, 2))\n",
    "# for threshold in thresholds:\n",
    "#     print(\"-----threshold {}-----\".format(threshold))\n",
    "#     y_pred = y_pred = [0 if _ < threshold else 1 for _ in y_pred_prob_0]\n",
    "#     print(me.accuracy_score(y_test, y_pred))\n",
    "print(\"-----accuracy-----\")\n",
    "print(me.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# 补充测试集\n",
    "extra_y_pred = optimized_classifier.predict(extra_df.iloc[:,1:])\n",
    "extra_y = extra_df.iloc[:, 0]\n",
    "print(\"------acccuracy------\")\n",
    "print(me.accuracy_score(extra_y, extra_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"num_leaves\" : 31,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": 'multi_logloss',\n",
    "    \"max_bin\": 50,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"is_unbalance\": True,\n",
    "    \"num_class\": 2\n",
    "    # \"silent\": 1,  # 信息输出设置成1则没有信息输出\n",
    "}\n",
    "\n",
    "print(\"training begins\")\n",
    "bst = lgb.train(params, train_data, num_round, valid_sets=[valid_data, valid_data_2, valid_data_3], early_stopping_rounds=10) \n",
    "bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "bst.save_model('mutationscore.model')\n",
    "print(\"training finishes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}